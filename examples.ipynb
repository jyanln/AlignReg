{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "examples.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First off, clone the github repository:"
      ],
      "metadata": {
        "id": "6_Hyjg_OVQu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://{username}:{token}@github.com/jyanln/l2-augmentation-regularization"
      ],
      "metadata": {
        "id": "egNMwA92VRcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow\n",
        "To use this package with tensorflow, first we must import the relevant packages and files:"
      ],
      "metadata": {
        "id": "-wghOwJnVSgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "sys.path.append('l2-augmentation-regularization/src')\n",
        "from tf_training import *"
      ],
      "metadata": {
        "id": "OcJf29T-WSK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by setting up a tensorflow module as we would normally:"
      ],
      "metadata": {
        "id": "lE5-VGO3XN1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and normalize the CIFAR10 dataset:\n",
        "(ds_train, ds_test), ds_info = tfds.load('cifar10',\n",
        "                                         split=['train', 'test'],\n",
        "                                         shuffle_files=True,\n",
        "                                         as_supervised=True,\n",
        "                                         with_info=True\n",
        "                                        )\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ds_train.map(normalize_img)\n",
        "ds_test = ds_test.map(normalize_img)\n",
        "\n",
        "# Batch the datasets\n",
        "batch_size = 128\n",
        "ds_train = ds_train.batch(batch_size)\n",
        "ds_test = ds_test.batch(batch_size)\n",
        "\n",
        "# Create the model and parameters needed to run the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(0.001)\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "C7oD_qJiYhCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we set a scalar factor to control the regularization, as well as specify the augmentation functions that should be used for the process. Then, we can call the provided `tf_train` function. In tensorflow, here we can adjust whether the augmentations are processed eagerly or lazily.\n"
      ],
      "metadata": {
        "id": "KDa3YNrwaT5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set regularization factor\n",
        "l2_lambda = 0.01\n",
        "\n",
        "# Set augmentation functions\n",
        "# Here we use the default list from the augment module\n",
        "augmentations = tf_default_augmentations\n",
        "\n",
        "# Pass variables into training function\n",
        "acc, loss_hist = tf_train(ds_train,\n",
        "                          ds_test,\n",
        "                          model, \n",
        "                          optimizer, \n",
        "                          epochs, \n",
        "                          loss,\n",
        "                          l2_lambda,\n",
        "                          augmentations=tf_default_augmentations,\n",
        "                          lazy_augmentation=True)"
      ],
      "metadata": {
        "id": "cA_tbMaVar9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can plot the output to view the accuracy history:\n",
        "\n"
      ],
      "metadata": {
        "id": "pouXdOdYcZb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(acc, color='r')\n",
        "plt.xlabel('Batch#')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PikIVA37imMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch\n",
        "To use this package with tensorflow, first we must import the relevant packages and files:"
      ],
      "metadata": {
        "id": "lvf9r3x6mikA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "sys.path.append('l2-augmentation-regularization/src')\n",
        "from augment import *\n",
        "from torch_training import *"
      ],
      "metadata": {
        "id": "OgYXMr7hmqkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by setting up a tensorflow module as we would normally:"
      ],
      "metadata": {
        "id": "uAolHD6Fm4vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and normalize the CIFAR10 dataset:\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Create the model and parameters needed to run the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "nEHqJmt7nHlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Pytorch, we modify the training dataset using the list of augmentation functions that we wish to use. We also specify here whether we wish the dataset to be evaluated eagerly or lazily."
      ],
      "metadata": {
        "id": "hvCkR9kinf3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we use the default list from the augment module\n",
        "augmentations = torch_default_augmentations\n",
        "\n",
        "trainset = AugmentedDataset(trainset, augmentations, lazy_augmentation=True)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "XN4cPT-5n0no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we set a scalar factor to control the regularization, as well as specify the augmentation functions that should be used for the process. Then, we can call the provided `tf_train` function:"
      ],
      "metadata": {
        "id": "j9Zcfi31qRwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set regularization factor\n",
        "l2_lambda = 0.01\n",
        "\n",
        "# Pass variables into training function\n",
        "acc, loss_hist = torch_train(trainloader,\n",
        "                             testloader,\n",
        "                             net,\n",
        "                             optimizer,\n",
        "                             epochs,\n",
        "                             loss,\n",
        "                             l2_lambda)"
      ],
      "metadata": {
        "id": "dViEQcTQqiPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can plot the output to view the accuracy history:"
      ],
      "metadata": {
        "id": "PvYcNcZTqq6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(acc, color='r')\n",
        "plt.xlabel('Batch#')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kLPa14baqrXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
